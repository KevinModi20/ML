{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO5rE6pn2rP5bOcwKb+cYFB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"c5CKkW-CMf85"},"outputs":[],"source":["# A Case study on logistic regression and write a program for logistic regression\n","# Importing libraries\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import warnings\n","warnings.filterwarnings( \"ignore\" )\n","# to compare our model's accuracy with sklearn model\n","from sklearn.linear_model import LogisticRegression"]},{"cell_type":"code","source":["# Logistic Regression\n","class LogitRegression() :\n","\tdef __init__( self, learning_rate, iterations ) :\t\t\n","\t\tself.learning_rate = learning_rate\t\t\n","\t\tself.iterations = iterations\n","\t\t\n","\t# Function for model training\t\n","\tdef fit( self, X, Y ) :\t\t\n","\t\t# no_of_training_examples, no_of_features\t\t\n","\t\tself.m, self.n = X.shape\t\t\n","\t\t# weight initialization\t\t\n","\t\tself.W = np.zeros( self.n )\t\t\n","\t\tself.b = 0\t\t\n","\t\tself.X = X\t\t\n","\t\tself.Y = Y\n","\t\n","\n","\t\t# gradient descent learning\n","\t\t\n","\t\tfor i in range( self.iterations ) :\t\t\t\n","\t\t\tself.update_weights()\t\t\t\n","\t\treturn self\n","\t\n","\t# Helper function to update weights in gradient descent\n","\t\n","\tdef update_weights( self ) :\t\t\n","\t\tA = 1 / ( 1 + np.exp( - ( self.X.dot( self.W ) + self.b ) ) )\n","\t\t\n","\t\t# calculate gradients\t\t\n","\t\ttmp = ( A - self.Y.T )\t\t\n","\t\ttmp = np.reshape( tmp, self.m )\t\t\n","\t\tdW = np.dot( self.X.T, tmp ) / self.m\t\t\n","\t\tdb = np.sum( tmp ) / self.m\n","\t\t\n","\t\t# update weights\t\n","\t\tself.W = self.W - self.learning_rate * dW\t\n","\t\tself.b = self.b - self.learning_rate * db\n","\t\t\n","\t\treturn self\n","\t# Hypothetical function h( x )\n","\t\n","\tdef predict( self, X ) :\t\n","\t\tZ = 1 / ( 1 + np.exp( - ( X.dot( self.W ) + self.b ) ) )\t\t\n","\t\tY = np.where( Z > 0.5, 1, 0 )\t\t\n","\t\treturn Y"],"metadata":{"id":"kev3pHIGMtbc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main() :\n","\t\n","\t# Importing dataset\t\n","\tdf = pd.read_csv( \"diabetes.csv\" )\n","\tX = df.iloc[:,:-1].values\n","\tY = df.iloc[:,-1:].values\n","\t\n","\t# Splitting dataset into train and test set\n","\tX_train, X_test, Y_train, Y_test = train_test_split(\n","\tX, Y, test_size = 1/3, random_state = 0 )\n","\t\n","\t# Model training\t\n","\tmodel = LogitRegression( learning_rate = 0.01, iterations = 1000 )\n","\t\n","\tmodel.fit( X_train, Y_train )\t\n","\tmodel1 = LogisticRegression()\t\n","\tmodel1.fit( X_train, Y_train)\n","\t\n","\t# Prediction on test set\n","\tY_pred = model.predict( X_test )\t\n","\tY_pred1 = model1.predict( X_test )\n","\t\n","\t# measure performance\t\n","\tcorrectly_classified = 0\t\n","\tcorrectly_classified1 = 0\n","\t\n","\t# counter\t\n","\tcount = 0\t\n","\tfor count in range( np.size( Y_pred ) ) :\n","\t\tif Y_test[count] == Y_pred[count] :\t\t\t\n","\t\t\tcorrectly_classified = correctly_classified + 1\t\t\n","\t\tif Y_test[count] == Y_pred1[count] :\t\t\t\n","\t\t\tcorrectly_classified1 = correctly_classified1 + 1\t\n","\t\tcount = count + 1\n","\t\t\n","\tprint( \"Accuracy on test set by our model\t : \", (\n","\tcorrectly_classified / count ) * 100 )\n","\tprint( \"Accuracy on test set by sklearn model : \", (\n","\tcorrectly_classified1 / count ) * 100 )\n"],"metadata":{"id":"wBh7hKDuM3F3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\" :\t\n","\tmain()"],"metadata":{"id":"f28OA_4-M7TP"},"execution_count":null,"outputs":[]}]}